{"cells":[{"cell_type":"code","execution_count":3,"id":"32a2b778","metadata":{"id":"32a2b778","executionInfo":{"status":"ok","timestamp":1690064660349,"user_tz":420,"elapsed":167,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["# MAIN PACKAGES\n","import pandas as pd\n","import numpy as np\n","import sklearn\n","from tqdm import tqdm\n","\n","# MODEL VALIDATION\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","#RFR SPECIFIC\n","from sklearn.ensemble import RandomForestRegressor\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93tRHKJ-rOtI","executionInfo":{"status":"ok","timestamp":1690064676241,"user_tz":420,"elapsed":14910,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}},"outputId":"449ddfca-68d7-41f0-8580-07dc86f7b668"},"id":"93tRHKJ-rOtI","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["This notebook trains + tests each encoding library to find the best perfoming encoding amino acid property matricies. These Matricies will be used to generate final predictions on witheld test set and novel mutant library.\n","\n","\n","Inputs:\n","1.   Combined Dataset Path (String)\n","2.   Desired Name of Dependent Variable (String)\n","3.   len(Sequence of Base Variant) (int)\n","4.   Property Matrix Path (String)\n","\n","Outputs:\n","df_RFR\n","*   Dataframe (2 columns: Encoding Dataset Column + R Squared Achieved Column)\n","*   Saved as RFR_Encoding_Datasets.csv in the RFR/ Sub Folder\n"],"metadata":{"id":"R23O69bDrTnv"},"id":"R23O69bDrTnv"},{"cell_type":"code","execution_count":5,"id":"b5294a78","metadata":{"id":"b5294a78","executionInfo":{"status":"ok","timestamp":1690064678412,"user_tz":420,"elapsed":165,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["mutation_datset_path = '/content/drive/MyDrive/ml_paper_ipnys/backend_data/combined_dataset.csv'\n","len_sequence_of_base_variant = 451 # len(Sequence of Base Variant)\n","dependent_variable = 'Fluor Decay' # Desired Name of Dependent Variable\n","encoding_dataset_path = '/content/drive/MyDrive/ml_paper_ipnys/backend_data/full_property_matrix.csv'\n","parent_folder_path = '2023-07-22-ensemble-run/' #should be created in wrapper"]},{"cell_type":"code","execution_count":6,"id":"20dcd7f9","metadata":{"id":"20dcd7f9","executionInfo":{"status":"ok","timestamp":1690064682247,"user_tz":420,"elapsed":833,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["data = pd.read_csv(mutation_datset_path)\n","position_cols = np.arange(0,len_sequence_of_base_variant)\n","position_cols = [str(i) for i in position_cols]\n","encoded_df = data[position_cols]\n","\n","x_train, x_test, y_train, y_test = train_test_split(encoded_df,\n","                                                    data[dependent_variable],\n","                                                    test_size=0.20,\n","                                                    random_state=42)"]},{"cell_type":"code","source":["# Read in encoding dataset\n","encoding_data = pd.read_csv(encoding_dataset_path, index_col = 0)"],"metadata":{"id":"jUK3eJ5OrwHw","executionInfo":{"status":"ok","timestamp":1690064685458,"user_tz":420,"elapsed":364,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"id":"jUK3eJ5OrwHw","execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"id":"e89ffff7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e89ffff7","executionInfo":{"status":"ok","timestamp":1690085215396,"user_tz":420,"elapsed":20523783,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}},"outputId":"1abc8202-36c5-4c21-a767-8595d24594c2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Property Datasets Encoded:: 100%|██████████| 566/566 [5:42:03<00:00, 36.26s/it]\n"]}],"source":["#initialize output dataframe\n","df_RFR = pd.DataFrame()\n","\n","#Iterate through every encoding dataset + train model +\n","#record performance\n","for AA_property_dataset in tqdm(encoding_data.columns[1:],\n","                                desc = 'Property Datasets Encoded:'):\n","\n","    # Make the train/test be on a copy to ensure there\n","    #is no data overwriting within loop\n","    x_train_copy = x_train.copy()\n","    x_test_copy = x_test.copy()\n","\n","    #extract encoding data for specific iteration\n","    volume_dict = {'Amino Acid Code': encoding_data[encoding_data.columns[0]],\n","                   AA_property_dataset: encoding_data[AA_property_dataset]}\n","    volume_data = pd.DataFrame(volume_dict)\n","\n","\n","    #Some encoding datasets contain NaNs, I skip these\n","    #datasets since theyre incomplete\n","    #Next three lines check the encoding data for NaNs\n","    df = list(volume_data.iloc[:,1].values)\n","    T = np.isnan(df)\n","    TF = True in T\n","\n","    #If there is no NaN, perform model training\n","    if TF == False:\n","\n","        #initialize list to append to throughout training\n","        interlist = []\n","\n","        #Use volume_data as a codex to translate sequence data...\n","        # amino acids will translate to float type data\n","        col_title = volume_data.columns[1]\n","        for row, sample in enumerate(volume_data['Amino Acid Code']):\n","            amino = sample\n","            replacement_value = float(volume_data[col_title].iloc[row])\n","            x_train_copy = x_train_copy.replace(amino,replacement_value)\n","            x_test_copy = x_test_copy.replace(amino,replacement_value)\n","\n","\n","        #Initialize Model and fit data to extract feature importances\n","        model = RandomForestRegressor()\n","        model.fit(x_train_copy,y_train)\n","        feat_importances = pd.Series(model.feature_importances_,\n","                                     index=x_train_copy.columns)\n","\n","\n","        ##Hyper Parameter Tuning\n","        #Grid Search approach, I test every iteration to find the best\n","        #combination of neighbors and features\n","\n","        #Initialize the output lists to append to\n","        n_est = []\n","        test_r2s = []\n","\n","        #For 21 possible best features\n","        for l in range(21):\n","            if l > 0:\n","                cols = list(feat_importances.nlargest(l).index)\n","\n","                #extract l features from X_train/X_test\n","                x_train_copy_ = x_train_copy[cols]\n","                x_test_copy_ = x_test_copy[cols]\n","\n","                #Initialize list of estimators to test\n","                n_estimators = [10, 15, 20, 25, 30, 35, 40,\n","                                45, 50, 55, 65, 75, 85, 100]\n","\n","                #Initialize the output lists to append to\n","                n_mse_list = []\n","                n_r2_list = []\n","\n","                for estimators in n_estimators:\n","                    #initialize model with n number of estimators\n","                    clf_RF = RandomForestRegressor(n_estimators=estimators,\n","                                                   random_state=42)\n","                    #Fit the Train data\n","                    clf_RF.fit(x_train_copy_, y_train)\n","                    #Predict the Test set and generate metrics of fit\n","                    y_RF = clf_RF.predict(x_test_copy_)\n","                    n_mse_list.append(mean_squared_error(y_test, y_RF))\n","                    n_r2_list.append(sklearn.metrics.r2_score(y_test, y_RF))\n","\n","                #find which number of estimators led to the best R2 for\n","                # the test set\n","                best_est = n_estimators[n_mse_list.index(min(n_mse_list))]\n","                n_est.append(best_est)\n","                test_r2s.append(np.mean(n_r2_list))\n","\n","        #find which iteration led to the greatest R2\n","        #est_best is the number of estimators when the\n","        #max R2 occured\n","        best_feats = test_r2s.index(max(test_r2s))\n","        est_best = n_est[best_feats]\n","\n","        #similarly, extract the optimal number of features\n","        #by extracting the number of features that led to the\n","        #greatest R2\n","        cols = list(feat_importances.nlargest(best_feats + 1).index)\n","        x_train_copy_ = x_train_copy[cols]\n","        x_test_copy_ = x_test_copy[cols]\n","\n","        #initialize new model + fit data\n","        clf = RandomForestRegressor(n_estimators = est_best,random_state = 42)\n","        clf.fit(x_train_copy_, y_train)\n","\n","        #Create cross validation prediction\n","        y_pred = clf.predict(x_test_copy_)\n","\n","        #Save the overall R2 for the tuned model\n","        r2 = sklearn.metrics.r2_score(y_test, y_pred)\n","\n","        #append the data to a dataframe for export\n","        inter_df= pd.DataFrame({'Encoding Dataset': [AA_property_dataset[-11:]],\n","                                'Test Set R Squared' : [r2]})\n","\n","        df_RFR = pd.concat([df_RFR,inter_df],ignore_index = True)\n","\n","df_RFR = df_RFR.sort_values(by='Test Set R Squared', ascending = False)\n","df_RFR.to_csv(parent_folder_path + 'RFR/RFR_Encoding_Datasets.csv')\n","\n","\n","assert len(df_RFR) == 553\n","assert df_RFR['Test Set R Squared'].iloc[0] >= df_RFR['Test Set R Squared'].iloc[1]"]},{"cell_type":"code","execution_count":10,"id":"99f4e11f","metadata":{"id":"99f4e11f","executionInfo":{"status":"ok","timestamp":1690085215397,"user_tz":420,"elapsed":7,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["##FOR COLAB ONLY\n","df_RFR.to_csv('/content/drive/MyDrive/ml_paper_ipnys/RFR_Encoding_Datasets.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}