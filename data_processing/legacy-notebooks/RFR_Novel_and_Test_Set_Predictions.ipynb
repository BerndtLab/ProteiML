{"cells":[{"cell_type":"code","execution_count":1,"id":"32a2b778","metadata":{"id":"32a2b778","executionInfo":{"status":"ok","timestamp":1690090979109,"user_tz":420,"elapsed":3134,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["# MAIN PACKAGES\n","import pandas as pd\n","import numpy as np\n","import sklearn\n","from tqdm import tqdm\n","\n","# MODEL VALIDATION\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","#RFR SPECIFIC\n","from sklearn.ensemble import RandomForestRegressor\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93tRHKJ-rOtI","executionInfo":{"status":"ok","timestamp":1690091003938,"user_tz":420,"elapsed":24833,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}},"outputId":"4827e859-3327-471a-b961-543de5f9ceb2"},"id":"93tRHKJ-rOtI","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["This notebook reads the top encoding datasets and uses them to form predictions on the Novel mutant library and saves predictions of the test set for cross validation.\n","\n","\n","Inputs:\n","1.   Combined Dataset Path (String)\n","2.   Desired Name of Dependent Variable (String)\n","3.   len(Sequence of Base Variant) (int)\n","4.   Property Matrix Path (String)\n","5.   path to parent folder\n","5.   path to RFR/RFR_Encoding_Datasets.csv\n","6.   path to Novel mutant library\n","\n","Outputs:\n","5 novel prediction notebooks\n","*   once per top five encoding datasets, the final predictions of the novel library are saved.\n","*   Saved as dataset_name_novel_library_predictions.csv in the RFR/ Sub Folder\n","\n","5 test set prediction notebooks\n","*   once per top five encoding datasets, the final predictions of test set are saved.\n","*   Saved as dataset_name_test_set_predictions.csv in the RFR/ Sub Folder\n"],"metadata":{"id":"R23O69bDrTnv"},"id":"R23O69bDrTnv"},{"cell_type":"code","execution_count":9,"id":"b5294a78","metadata":{"id":"b5294a78","executionInfo":{"status":"ok","timestamp":1690091105194,"user_tz":420,"elapsed":3,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["mutation_datset_path = '/content/drive/MyDrive/ml_paper_ipnys/backend_data/combined_dataset.csv'\n","dependent_variable = 'Fluor Decay' # Desired Name of Dependent Variable\n","len_sequence_of_base_variant = 451 # len(Sequence of Base Variant)\n","encoding_dataset_path = '/content/drive/MyDrive/ml_paper_ipnys/backend_data/full_property_matrix.csv'\n","parent_folder_path = '2023-07-22-ensemble-run/' #should be created in wrapper\n","\n","# specific for this notebook\n","novel_library_path = '/content/drive/MyDrive/ml_paper_ipnys/backend_data/novel_mutant_library.csv'\n","top_encoding_dataset_path = '/content/drive/MyDrive/ml_paper_ipnys/RFR_Encoding_Datasets.csv'"]},{"cell_type":"code","execution_count":4,"id":"20dcd7f9","metadata":{"id":"20dcd7f9","executionInfo":{"status":"ok","timestamp":1690091005098,"user_tz":420,"elapsed":617,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[],"source":["data = pd.read_csv(mutation_datset_path)\n","position_cols = np.arange(0,len_sequence_of_base_variant)\n","position_cols = [str(i) for i in position_cols]\n","encoded_df = data[position_cols]\n","\n","x_train, x_test, y_train, y_test = train_test_split(encoded_df,\n","                                                    data[dependent_variable],\n","                                                    test_size=0.20,\n","                                                    random_state=42)"]},{"cell_type":"code","source":["# Read in encoding dataset\n","encoding_data = pd.read_csv(encoding_dataset_path, index_col = 0)"],"metadata":{"id":"jUK3eJ5OrwHw","executionInfo":{"status":"ok","timestamp":1690091008736,"user_tz":420,"elapsed":430,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"id":"jUK3eJ5OrwHw","execution_count":5,"outputs":[]},{"cell_type":"code","source":["#read in the Library of Novel Variant Sequences\n","data_pred = pd.read_csv(novel_library_path, index_col = 0)\n","\n","#Remove Any Duplicate Sequences (redundancies with 7s)\n","data_pred = data_pred.drop_duplicates(subset = data_pred.columns[2:-2],\n","                                      keep='first',\n","                                      inplace=False, ignore_index=False)\n","dropped = data_pred[data_pred.duplicated(subset = data_pred.columns[2:-2])]\n","\n","#tests\n","assert len(dropped) == 0"],"metadata":{"id":"Rr1OuT9r9xla","executionInfo":{"status":"ok","timestamp":1690091250112,"user_tz":420,"elapsed":695,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"id":"Rr1OuT9r9xla","execution_count":13,"outputs":[]},{"cell_type":"code","source":["# read in the encoding datasets:\n","top_encoding_data = pd.read_csv(top_encoding_dataset_path,index_col= 0)\n","top_encoding_data = top_encoding_data.sort_values(by = 'Test Set R Squared',\n","                                                          ascending = False)\n","names = [e[-10:] for e in top_encoding_data['Encoding Dataset'][0:5]]\n","\n","\n","##tests\n","assert np.shape(names) == (5,)\n","assert top_encoding_data['Test Set R Squared'].iloc[0]>=top_encoding_data['Test Set R Squared'].iloc[1]"],"metadata":{"id":"TPLk_KQR907g","executionInfo":{"status":"ok","timestamp":1690091108608,"user_tz":420,"elapsed":364,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"id":"TPLk_KQR907g","execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":10,"id":"e89ffff7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e89ffff7","outputId":"48d05432-3c75-4309-a0ec-8dc5e16980dc","executionInfo":{"status":"ok","timestamp":1690069727458,"user_tz":420,"elapsed":165855,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Property Datasets Encoded:: 100%|██████████| 5/5 [02:45<00:00, 33.12s/it]\n"]}],"source":["#initialize output dataframe\n","df_RFR = pd.DataFrame()\n","save_path = parent_folder_path +'RFR/'\n","\n","#Iterate through every encoding dataset + train model +\n","#record performance\n","for AA_property_dataset in tqdm(names,\n","                                desc = 'Property Datasets Encoded:'):\n","\n","    #format to match column names\n","    column_names_ref = 'AAindex: '+AA_property_dataset\n","\n","    # Make the train/test be on a copy to ensure there\n","    #is no data overwriting within loop\n","    x_train_copy = x_train.copy()\n","    x_test_copy = x_test.copy()\n","    x_pred_copy = data_pred.copy()\n","\n","    #extract encoding data for specific iteration\n","    volume_dict = {'Amino Acid Code': encoding_data[encoding_data.columns[0]],\n","                   AA_property_dataset: encoding_data[column_names_ref]}\n","    volume_data = pd.DataFrame(volume_dict)\n","\n","\n","    #Some encoding datasets contain NaNs, I skip these\n","    #datasets since theyre incomplete\n","    #Next three lines check the encoding data for NaNs\n","    df = list(volume_data.iloc[:,1].values)\n","    T = np.isnan(df)\n","    TF = True in T\n","\n","    #If there is no NaN, perform model training\n","    if TF == False:\n","\n","        #initialize list to append to throughout training\n","        interlist = []\n","\n","        #Use volume_data as a codex to translate sequence data...\n","        # amino acids will translate to float type data\n","        col_title = volume_data.columns[1]\n","        for row, sample in enumerate(volume_data['Amino Acid Code']):\n","            amino = sample\n","            replacement_value = float(volume_data[col_title].iloc[row])\n","            x_train_copy = x_train_copy.replace(amino,replacement_value)\n","            x_test_copy = x_test_copy.replace(amino,replacement_value)\n","            x_pred_copy = x_pred_copy.replace(amino,replacement_value)\n","\n","\n","        #Initialize Model and fit data to extract feature importances\n","        model = RandomForestRegressor()\n","        model.fit(x_train_copy,y_train)\n","        feat_importances = pd.Series(model.feature_importances_,\n","                                     index=x_train_copy.columns)\n","\n","\n","        ##Hyper Parameter Tuning\n","        #Grid Search approach, I test every iteration to find the best\n","        #combination of neighbors and features\n","\n","        #Initialize the output lists to append to\n","        n_est = []\n","        test_r2s = []\n","\n","        #For 21 possible best features\n","        for l in range(21):\n","            if l > 0:\n","                cols = list(feat_importances.nlargest(l).index)\n","\n","                #extract l features from X_train/X_test\n","                x_train_copy_ = x_train_copy[cols]\n","                x_test_copy_ = x_test_copy[cols]\n","\n","                #Initialize list of estimators to test\n","                n_estimators = [10, 15, 20, 25, 30, 35, 40,\n","                                45, 50, 55, 65, 75, 85, 100]\n","\n","                #Initialize the output lists to append to\n","                n_mse_list = []\n","                n_r2_list = []\n","\n","                for estimators in n_estimators:\n","                    #initialize model with n number of estimators\n","                    clf_RF = RandomForestRegressor(n_estimators=estimators,\n","                                                   random_state=42)\n","                    #Fit the Train data\n","                    clf_RF.fit(x_train_copy_, y_train)\n","                    #Predict the Test set and generate metrics of fit\n","                    y_RF = clf_RF.predict(x_test_copy_)\n","                    n_mse_list.append(mean_squared_error(y_test, y_RF))\n","                    n_r2_list.append(sklearn.metrics.r2_score(y_test, y_RF))\n","\n","                #find which number of estimators led to the best R2 for\n","                # the test set\n","                best_est = n_estimators[n_mse_list.index(min(n_mse_list))]\n","                n_est.append(best_est)\n","                test_r2s.append(np.mean(n_r2_list))\n","\n","        #find which iteration led to the greatest R2\n","        #est_best is the number of estimators when the\n","        #max R2 occured\n","        best_feats = test_r2s.index(max(test_r2s))\n","        est_best = n_est[best_feats]\n","\n","        #similarly, extract the optimal number of features\n","        #by extracting the number of features that led to the\n","        #greatest R2\n","        cols = list(feat_importances.nlargest(best_feats + 1).index)\n","        x_train_copy_ = x_train_copy[cols]\n","        x_test_copy_ = x_test_copy[cols]\n","        x_pred_copy_ = x_pred_copy[cols]\n","\n","        #initialize new model + fit data\n","        clf = RandomForestRegressor(n_estimators = est_best,random_state = 42)\n","        clf.fit(x_train_copy_, y_train)\n","\n","        #Create cross validation prediction\n","        y_pred = clf.predict(x_test_copy_)\n","\n","        #Save the overall R2 for the tuned model\n","        r2 = sklearn.metrics.r2_score(y_test, y_pred)\n","\n","        #append the data to a dataframe for export\n","        inter_df= pd.DataFrame({'Encoding Dataset': [AA_property_dataset[-11:]],\n","                                'Test Set R Squared' : [r2]})\n","\n","        df_RFR = pd.concat([df_RFR,inter_df],ignore_index = True)\n","\n","\n","        # Novel Library Predictions\n","        y_pred_new = clf.predict(x_pred_copy_)\n","        data_pred[dependent_variable +' Predicted'] = y_pred_new\n","        data_pred = data_pred.sort_values(by = dependent_variable +' Predicted',\n","                                          ascending = False)\n","        newmut = pd.DataFrame(data_pred)\n","\n","\n","        #Test Set Predictions\n","        test_set_df = pd.DataFrame()\n","        test_set_df['True'] = y_test\n","        test_set_df['Predicted'] = y_pred\n","\n","        #file saving\n","        newmut.to_csv(save_path + AA_property_dataset+'_novel_library_predictions.csv')\n","        test_set_df.to_csv(save_path + AA_property_dataset+'_test_set_predictions.csv')\n","\n","\n","        #FOR COLAB ONLY\n","        save_path = '/content/drive/MyDrive/ml_paper_ipnys/RFR_'\n","        newmut.to_csv(save_path + AA_property_dataset+'_novel_library_predictions.csv')\n","        test_set_df.to_csv(save_path + AA_property_dataset+'_test_set_predictions.csv')\n","\n","assert len(df_RFR) == 5\n","assert len(data_pred) == len(newmut)\n","assert len(test_set_df) == len(x_test)"]},{"cell_type":"code","source":["df_RFR.sort_values(by='Test Set R Squared', ascending = False)"],"metadata":{"id":"m8r23brus180","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1690069727459,"user_tz":420,"elapsed":7,"user":{"displayName":"Sarah Wait","userId":"14055064724979742320"}},"outputId":"2b1342e1-8fcc-4040-8d8d-bbfe43128d77"},"id":"m8r23brus180","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  Encoding Dataset  Test Set R Squared\n","0       BASU050103            0.847938\n","4       ZIMJ680103            0.841128\n","3       GEIM800104            0.839672\n","1       HUTJ700101            0.822306\n","2       ISOY800107            0.778070"],"text/html":["\n","\n","  <div id=\"df-dec477e6-2c76-4cb3-9590-6fc4ae8bfb95\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Encoding Dataset</th>\n","      <th>Test Set R Squared</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BASU050103</td>\n","      <td>0.847938</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ZIMJ680103</td>\n","      <td>0.841128</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GEIM800104</td>\n","      <td>0.839672</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HUTJ700101</td>\n","      <td>0.822306</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ISOY800107</td>\n","      <td>0.778070</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dec477e6-2c76-4cb3-9590-6fc4ae8bfb95')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-b52d5d3a-51b1-48d7-97ff-39fb8d7d374d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b52d5d3a-51b1-48d7-97ff-39fb8d7d374d')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-b52d5d3a-51b1-48d7-97ff-39fb8d7d374d button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dec477e6-2c76-4cb3-9590-6fc4ae8bfb95 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dec477e6-2c76-4cb3-9590-6fc4ae8bfb95');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":11}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}